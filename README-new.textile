

h3. Getting Started

Follow the normal knife setup. If you can use the normal knife bootstrap
commands to launch a machine, you're ready to start. Make sure you've added the
cluster chef cookbooks/ and site-cookbooks/ directories to your knife.rb.

Upload everything:

  <pre>
    cd ~/path/to/cluster_chef
    knife cookbook upload --all
    for foo in roles/*.rb ; do knife role from file $foo & sleep 1 ; done
    rake load_data_bags
  </pre>

Apparently our databags injection rake script is now deprecated; if you can't
get it to work, you'll see some warnings from the cluster_ebs_volumes but
otherwise don't sweat it...

h3. Cluster chef knife commands

In your knife.rb, add a line

  <pre>
    cluster_chef_path '/path/to/your/cluster_chef'
  </pre>

and of course set your cloud keys:

  <pre>
    knife[:aws_access_key_id]      = "XXXXXXXXXXX"
    knife[:aws_secret_access_key]  = "XXXXXXXXXXXXXX"
    knife[:aws_account_id]         = "777777777777"
  </pre>

h3. Stupid Surgical bits

Until I figure out how to integrate things into knife, you have to symlink the
new commands into chef's lib/chef/knife directory, and symlink the bootstrap
templates into chef's lib/chef/knife/bootstrap directory.

  <pre>
    # modify these two lines to your system
    CHEF_GEM_PATH=/usr/local/lib/ruby/gems/*/gems/chef-0.9.12 
    CLUSTER_CHEF_PATH=$HOME/path/to/cluster_chef

    sudo ln -s $CLUSTER_CHEF_PATH/lib/cluster_chef/knife/*.rb $CHEF_GEM_PATH/lib/chef/knife/
    sudo ln -s $CLUSTER_CHEF_PATH/lib/cluster_chef/knife/bootstrap/*.erb $CHEF_GEM_PATH/lib/chef/knife/bootstrap/
  </pre>


h3. knife cluster launch

Now if you type @knife cluster launch@ you should see it found the new scripts:

  <pre>
  ** CLUSTER COMMANDS **
  knife cluster launch CLUSTER_NAME FACET_NAME (options)
  knife cluster show CLUSTER_NAME FACET_NAME (options)
  </pre>

Go ahead and launch a cluster:

  <pre>
  knife cluster launch demohadoop master --bootstrap
  </pre>

It will kick off a node and then bootstrap it.  By the time it's done, you
should be able to see the hadoop dashboard (follow the instructions for proxy
setup).
  
h3. Gotchas

* The initial startup is still finicky, but is at least down to only two passes for hadoop:

  <pre>
    for foo in hadoop-0.20-{datanode,namenode,tasktracker,jobtracker,secondarynamenode} ; do sudo service $foo stop ; done
    sudo chef-client
  </pre>

* For hbase, still dialing it in but there's also this:  
  
  <pre>
    sudo -u hdfs hadoop fs -chown -R hbase:hbase /hadoop/hbase
    sudo chef-client
  </pre>

* Once the master runs to completion with all daemons started, remove the
  hadoop_initial_bootstrap recipe from its run_list. (Note that you may have to
  edit the runlist on the machine itself depending on how you bootstrapped the
  node).

* For problems starting NFS server on ubuntu maverick systems, read, understand
  and then run /tmp/fix_nfs_on_maverick_amis.sh -- See
  "this thread for more":http://fossplanet.com/f10/[ec2ubuntu]-not-starting-nfs-kernel-daemon-no-support-current-kernel-90948/

h3. Zero-bootstrap, fire and forget cluster launch!

* Register for Amazon SimpleDB. (Although you do need a credit card, there's no conceivable way broham will approach the free limit.)
* You'll have to run the following one-time command:
  <pre>
    sudo gem install broham configliere right_aws
    ruby -rubygems -e 'require "broham"; Broham.establish_connection :access_key=>"YOUR_ACCESS_KEY", :secret_access_key=>"YOUR_KEY"; Broham.create_domain'
  <pre>
* Now you should be able to use broham:
  <pre>
    broham-register `hostname`
  </pre>
* To have it assign node names dynamically, se the client.rb script in cluster_chef/config as your /etc/chef/client.rb


