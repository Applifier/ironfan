h1. Big Data Cluster using Chef, Hadoop and Cassandra

h2. Overview

ClusterChef helps you create a scalable, efficient compute cluster in the cloud. It has recipes for Hadoop, Cassandra, NFS and more -- use as many or as few as you like. Go fluidly from experimentation to industrial-strength big data computing:

* A small 1-5 node cluster for development, or just for playing around with Hadoop and Cassandra
* A spot-priced, ebs-backed cluster for unattended computing at rock-bottom prices
* A large 30+ machine cluster with a shared NFS home dir, multiple EBS volumes per node, and Hadoop and Cassandra, tuned for the size of your machines.

h3. Advantages

* With Chef, you declare a final state for each node, not a procedure to follow. Adminstration is more efficient, robust and maintainable.
* You get a nice central dashboard to manage clients
* You can easily roll out configuration changes across all your machines
* Chef is actively developed and has well-written recipes for webservers, databases, development tools, and a ton of different software packages.
* Poolparty makes creating amazon cloud machines concise and easy: you can specify spot instances, ebs-backed volumes, disable-api-termination, and more.

h3. Components

* Hadoop
* NFS
* Persistent HDFS on EBS volumes
* Memory and storage settings defined for each machine type
* Cassandra
* Zookeeper (_in progress_)


h2. Getting started with a simple cluster

# Set up your local credentials and settings
# Launch cluster head (a combined chef server, hadoop master and nfs server; you can decouple these roles later)
# Gather credentials from the new server
# Launch slaves

h3. Initial Settings and Credentials

See detailed instructions in @notes/pt1-inital-settings-and-credentials.textile@

# Choose a name for your cluster, create the corresponding AWS keypair, and store its .pem file in ~/.hadoop-ec2/keypairs/
# Edit the poolparty.yaml file to match your setup

h3. Launch Chef server



<pre><code>
  cloud-start -n master  -c clouds/hadoop_clouds.rb
  # ... twiddle thumbs ...
  cloud-ssh   -n server  -c clouds/chef_clouds.rb
</code></pre>

You'll need to format the namenode and then restart the namenode (@sudo service hadoop-0.20-namenode restart@)

* Once the master node starts, try a couple slaves

<pre><code>
  cloud-start -n master  -c clouds/hadoop_clouds.rb
</code></pre>

h2. Set up knife and webui

EITHER: use your chef server to create a client called 'knife_user', and save the .pem file it generates as @~/.hadoop-ec2/keypairs/knife_user.pem@.

OR: On the server, run and answer as follows:

<pre><code>
  sudo knife configure -i
  Your chef server URL? http://chef.infochimps.com:4000
  Your client user name? knife_user
  Your validation client user name? chef-validator
  Path to a chef repository (or leave blank)? 
  WARN: Creating initial API user...
</code></pre>  
(note that on the chef server this must be run sudo so we can see the key files;  that the port is '4000' (server, not webui)

Now copy @~/.chef/knife_user.pem@ and @/etc/chef/validation.pem@ on the server
to (on your computer) @~/.hadoop-ec2/keypairs/knife_user.pem@ and
@~/.hadoop-ec2/keypairs/chef-validator.pem@ respectively; and @chmod og-rwx ~/.hadoop-ec2/keypairs/*.pem@

If you do @knife client list@ you should now see something like

<pre><code>
  [
    "chef-validator", 
    "chef-webui",
    "knife_user"
  ]
</code></pre>

h2. Stock the Chef Server

* Upload your roles and recipes:

<pre><code>
  cd PATH/TO/hadoop_cluster_chef
  for foo in roles/*.rb ; do echo $foo ; knife role from file $foo ; done
  knife cookbook upload --all
  rake load_data_bags
  rake load_data_bags
</code></pre>

h2. Play around with single-machine server

<pre><code>
cloud-start -n master -c clouds/hadoop_clouds.rb
</code></pre>

If you're using HDFS on the ephemeral drives, put the maintain_hadoop_cluster::reformat_scratch_disks, maintain_hadoop_cluster::format_namenode and maintain_hadoop_cluster::make_standard_hdfs_dirs.rb recipe in below hadoop and before hadoop_master. Nobody will question your courage if (rather than add them to the runlist) you just run the commands therein manually.

To see how things are hangin',

# On the master node
for foo in hadoop-0.20-{namenode,jobtracker,tasktracker,datanode,secondarynamenode} cassandra chef-client ; do sudo service $foo stop ; done

# On a worker node
for foo in hadoop-0.20-{tasktracker,datanode} cassandra chef-client nfs-kernel-server ; do sudo service $foo status ; done

