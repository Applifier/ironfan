h3. Prelaunch

Follow the normal knife setup. If you can use the normal knife bootstrap
commands to launch a machine, you're ready to start.

h3. Setup

  <pre>
    sudo gem install chef fog broham highline readline 
  </pre>
  
h4. AWS credentials

You need to make a cloud keypair, a secure key for communication with Amazon AWS
cloud. 

# Log in to the "AWS console":http://bit.ly/awsconsole and create a new
  keypair named @demohadoop@. Your browser will download the private half of it.

# Create a directory ~/.chef/keypairs/, and move the private key file you just
  downloaded to be ~/.chef/keypairs/demohadoop.pem. 

# Make the private key unsnoopable, or ssh will complain:

  <pre>  chmod 600 ~/.chef/keypairs/*.pem   </pre>
    

While you're there, also go to Account/Security Credentials and take a not of
your aws_access_key_id and aws_secret_access_key -- you'll need to add them to
your knife.rb as shown below.

h4. Knife setup

Clusterchef uses the 'knife' tool to control both chef and the cloud APIs.

We'll need to make some additions to its configuration file, which is probably
in ~/.chef/knife.rb. If you already have a @cookbook_path@ definition, just make
sure that @"#{cluster_chef_path}/cookbooks"@ and
@"#{cluster_chef_path}/site-cookbooks"@ appear in there too.

  <pre>
    # Type in the full path to your cluster_chef installation
    cluster_chef_path File.expand_path('~/ics/sysadmin/cluster_chef')
    # Type in the full path to the directory holding your cloud keypairs.
    keypair_path      File.expand_path("~/.chef/keypairs")

    # Make sure knife can find all your junk
    cookbook_path ["#{cluster_chef_path}/cookbooks", "#{cluster_chef_path}/site-cookbooks",] # and anything else you want

    # Set your AWS access credentials
    knife[:aws_access_key_id]      = "XXXXXXXXXXX"
    knife[:aws_secret_access_key]  = "XXXXXXXXXXXXX"
  </pre>

h4. Push to chef server

We need to send all the cookbooks and role to the chef server. Visit your
cluster_chef directory and run:

  <pre>
    cd ~/path/to/cluster_chef
    knife cookbook upload --all
    for foo in roles/*.rb ; do knife role from file $foo & sleep 1 ; done
  </pre>

h4. Stupid Surgical bits

Unfortunately the current version of chef doesn't have a plugin mechanism for
new commands. This means we have to do surgery on the knife itself... we'll just
symlink the new commands into chef's lib/chef/knife directory, and symlink the
bootstrap templates into chef's lib/chef/knife/bootstrap directory. Set the
path to your cluster_chef directory and run the following:

  <pre>
    CLUSTER_CHEF_PATH=$HOME/path/to/cluster_chef

    sudo ln -s $CLUSTER_CHEF_PATH/lib/cluster_chef/knife/*.rb            $(dirname `gem which chef`)/chef/knife/
    sudo ln -s $CLUSTER_CHEF_PATH/lib/cluster_chef/knife/bootstrap/*.erb $(dirname `gem which chef`)/chef/knife/bootstrap/
  </pre>

h3. Cluster chef knife commands


h4. knife cluster launch

Now if you type @knife cluster launch@ you should see it found the new scripts:

  <pre>
  ** CLUSTER COMMANDS **
  knife cluster launch CLUSTER_NAME FACET_NAME (options)
  knife cluster show CLUSTER_NAME FACET_NAME (options)
  </pre>

Go ahead and launch a cluster:

  <pre>
  knife cluster launch demohadoop master --bootstrap
  </pre>

It will kick off a node and then bootstrap it.  By the time it's done, you
should be able to see the hadoop dashboard (follow the instructions for proxy
setup).
  
h3. Gotchas

* The initial startup is still finicky, but is at least down to only two passes for hadoop:

  <pre>
    for foo in hadoop-0.20-{datanode,namenode,tasktracker,jobtracker,secondarynamenode} ; do sudo service $foo stop ; done
    sudo chef-client
  </pre>

* For hbase, still dialing it in but there's also this:  
  
  <pre>
    sudo -u hdfs hadoop fs -chown -R hbase:hbase /hadoop/hbase
    sudo chef-client
  </pre>

* Once the master runs to completion with all daemons started, remove the
  hadoop_initial_bootstrap recipe from its run_list. (Note that you may have to
  edit the runlist on the machine itself depending on how you bootstrapped the
  node).

* For problems starting NFS server on ubuntu maverick systems, read, understand
  and then run /tmp/fix_nfs_on_maverick_amis.sh -- See
  "this thread for more":http://fossplanet.com/f10/[ec2ubuntu]-not-starting-nfs-kernel-daemon-no-support-current-kernel-90948/

h3. Zero-bootstrap, fire and forget cluster launch!

* Register for Amazon SimpleDB. (Although you do need a credit card, there's no conceivable way broham will approach the free limit.)
* You'll have to run the following one-time command:
  <pre>
    sudo gem install broham configliere right_aws
    ruby -rubygems -e 'require "broham"; Broham.establish_connection :access_key=>"YOUR_ACCESS_KEY", :secret_access_key=>"YOUR_KEY"; Broham.create_domain'
  <pre>
* Now you should be able to use broham:
  <pre>
    broham-register `hostname`
  </pre>
* To have it assign node names dynamically, se the client.rb script in cluster_chef/config as your /etc/chef/client.rb


