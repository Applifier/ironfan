h1. Hadoop Cluster in the Cloud using Chef and Poolparty

h2. Overview

This will help you set up a scalable, efficient hadoop cluster on the Amazon EC2 cloud. It uses Poolparty to create instances, and Chef to provision them after start.

* Chef is declarative: you specify a final state for each node to reach, not a procedure to follow. Adminstration is more efficient, robust and maintainable.
* You get a nice central dashboard to manage clients
* You can easily roll out configuration changes across all your machines
* Chef is actively developed and has well-written recipes for a ton of different software packages.
* Poolparty makes creating amazon cloud machines concise and easy: you can specify spot instances, ebs-backed volumes, disable-api-termination, and more.

h2. Recommended cluster composition

We're going to target these two plausible cluster setups:

h3. Small, simple cluster ("Milo")

A modest, no-fuss cluster to get started:

* Master node acts as chef server, nfs server, hadoop master (namenode, secondarynamenode and jobtracker), hadoop worker.
* 0-5 worker nodes: nfs client, hadoop worker.
* All nodes are EBS-backed instances, sized large enough to hold the HDFS.
* Use non-spot pricing, but manage costs by starting/stopping instances when not in use. (Set 'ebs_delete_on_termination' to false and 'disable-api-termination' to true)

h3. Industrial-strength cluster with persistent HDFS ("Zaius")

A many-node cluster that can be spot priced (or frequently launched/terminated); uses persistent EBS volumes for the HDFS (much more efficient than S3).

* A standalone EBS-backed small instance acting as the chef server and nfs server. Can start/stop when not in use (set 'ebs_delete_on_termination' false and 'disable-api-termination' true) or use a reserved instance.
* Spot-priced master node (namenode, secondarynamenode and jobtracker) that is also a hadoop worker, nfs client, and cassandra node.
* 6-40 spot-priced worker nodes: hadoop worker, nfs client, cassandra node.
* All nodes are local-backed instances with EBS volumes attached at startup.
* You can shut down the cluster (or tolerate EC2 shutting it down if the spot price spikes) without harm to the HDFS. The NFS home dir lets you develop scripts on a small cluster and only spin up the big cluster for production jobs.
* For a larger cluster, you can turn off worker roles for the master node, and can specify the namenode and jobtracker to reside on different machines.
* You can specify any scale of instance depending on whether your job is IO-, CPU- or memory-intensive, and size master and worker nodes independently.

h2. Prerequisites

You should already be familiar with hadoop and with the Amazon cloud.  These scripts are meant to efficiently orchestrate many dependent packages, and the bugs are still being straightened out.

* Choose a name for your cluster. In this example, we'll use 'milo' for the small cluster and 'zaius' for the big cluster.
* Visit the "aws console":http://bit.ly/awsconsole and ensure you're registered for EC2 and SimpleDB. (You may have to click through a license agreement and check your email)
* Chef needs a good, durable domain name. Allocate an elastic IP; have your DNS server point both 'chef.yourdomain.com' and 'milo.yourdomain.com' at that elastic IP.

From now on, I'm going to just use 'chef.yourdomain.com', 'milo' and 'zaius' without apology, but substitute accordingly/

h2. Install dependencies

* Install these gems:

* chef
* configliere
* amazon-ec2
* broham

**NOTE**: Please use the "infochimps branch of poolparty":http://github.com/infochimps/poolparty and of "right_aws":http://github.com/infochimps/right_aws -- in the first case, for spot instance support, and in the second for the consistent-writes behavior that broham requires.

* infochimps-poolparty
* infochimps-right_aws

h2. Set up configuration files and credentials

At this point there's still a lot of moving parts. What I do is to make one directory for all the poolparty, chef, and other config files, and then use symlinks to make everyone happy. (Note: if you're already using the Cloudera hadoop-ec2 scripts some of this is already in place.)

<pre><code>
  mkdir ~/.hadoop-ec2
  mkdir ~/.hadoop-ec2/keypairs
  ln -nfs ~/.hadoop-ec2 ~/.poolparty
  ln -nfs ~/.hadoop-ec2 ~/.chef
</code></pre>

From this code repo dir, copy the template config files over

<pre><code>
  cd PATH/TO/hadoop_cluster_chef
  cp ./config/knife.rb                 ~/.hadoop-ec2/knife.rb                           
  cp ./config/poolparty-example.yaml   ~/.hadoop-ec2/poolparty.yaml 
  ln -nfs ~/.hadoop-ec2/poolparty.yaml ~/.hadoop-ec2/aws
</code></pre>

h2. Credentials



Now open ~/.hadoop-ec2/poolparty.yaml




===========================================================================


* On the chef server,
** authenticate your machine as a client
** create data bags called @cluster_ebs_volumes@ and @servers_info@ and (if using cassandra) @cassandra@

* Upload your roles and recipes:

<pre><code>
  for foo in roles/*.rb ; do echo $foo ; knife role from file $foo ; done
  knife cookbook upload --all
</code></pre>

(_There are probably a bunch more things in the middle here, please let me know what breaks between that step and the next_)

* Start the hadoop master node:

<pre><code>
  cloud-start -n master  -c clouds/hadoop_clouds.rb
  # ... twiddle thumbs ...
  cloud-ssh   -n server  -c clouds/chef_clouds.rb
</code></pre>

* You will probably need to kickstart chef-client a few times.  Log into the machine as @ubuntu@ user and

<pre><code>
  sudo service chef-client stop
  cd /etc/chef
  tail -f /etc/sv/chef-client/log/main/* &
  sudo chef-client
</code></pre>


* Once the master node starts, try a couple slaves

<pre><code>
  cloud-start -n master  -c clouds/hadoop_clouds.rb
</code></pre>

* Once the cluster works with no EBS volumes, then you should try defining 

h3. Caveats

If you're west, first run from the shell
<pre><code>
  export EC2_URL=https://us-west-1.ec2.amazonaws.com
<pre><code>

h3. Instance attributes: disable_api_termination and delete_on_termination

To set delete_on_termination to 'true' after the fact, run the following
<pre><code>
  ec2-modify-instance-attribute -v i-0704be6c --block-device-mapping /dev/sda1=vol-e98d2c80::true
</code></pre>
(You'll have to modify the instance and volume to suit)
  
If you set disable_api_termination to true, in order to terminate the node run
<pre><code>
  ec2-modify-instance-attribute -v i-0704be6c --disable-api-termination false
</code></pre>

h3. Tradeoffs of EBS-backed volumes

Be careful of the tradeoffs with EBS-backed volumes.

* _good_: You can start and stop instances -- don't pay for the compute from the end of that hour until you restart.
* _good_: It's way easier to tune up an AMI. (Then again, chef makes much of that unnecessary)
* _good_: You can make the volume survive even if the node is terminated (spot price is exceeded, machine crashes, etc).
* _good_: You can make a persistent HDFS without having to fart around attaching EBS volumes at startup. There are performance tradeoffs, though.
* _bad_: The disk is noticably slower. Make sure to point tempfiles and scratch space to the local drives. (The scripts currently handle most but not all of this).
* _bad_: The root volume counts against your quota for EBS volumes.
* _bad_: Starting more than six or so EBS-backed instances can cause AWS to shit a brick allocating all the volumes.

Refer to the standard setups described above.

h2. Information Sharing using simpleDB

* Make sure you log into the "aws console":http://bit.ly/awsconsole and check in as a SimpleDB user. (You have to click through a license agreement, it should approve you within minutes)
