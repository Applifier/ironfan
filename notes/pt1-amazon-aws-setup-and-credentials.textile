h1. Settings files and Credentials

* If you're using the west coast availability zone, first run @export EC2_URL=https://us-west-1.ec2.amazonaws.com@



h2. Install dependencies

Install these gems:

* chef
* configliere
* amazon-ec2
* broham

**NOTE**: Please use the "infochimps branch of poolparty":http://github.com/infochimps/poolparty for spot instance support and other tweaks

* infochimps-poolparty

h2. Set up configuration files and credentials

At this point there's still a lot of moving parts. What I do is to make one directory for all the poolparty, chef, and other config files, and then use symlinks to make everyone happy. (Note: if you're already using the Cloudera hadoop-ec2 scripts some of this is already in place.)

<pre><code>
  mkdir ~/.hadoop-ec2
  mkdir ~/.hadoop-ec2/keypairs
  ln -nfs ~/.hadoop-ec2 ~/.poolparty
  ln -nfs ~/.hadoop-ec2 ~/.chef
</code></pre>

From the chef server,

  sudo cat /etc/chef/validation.peb
  
And copy/paste the contents into

  ~/.chef/keypairs/chef-validator.pem

From this code repo dir, copy the template config files over.

<pre><code>
  cd PATH/TO/hadoop_cluster_chef
  cp ./config/knife.rb                 ~/.hadoop-ec2/knife.rb                           
  cp ./config/poolparty-example.yaml   ~/.hadoop-ec2/poolparty.yaml 
  ln -nfs ~/.hadoop-ec2/poolparty.yaml ~/.hadoop-ec2/aws
  # optional:
  ( cd ~/.hadoop-ec2 && git init && git add . && git commit -m "Initial commit" )
</code></pre>

In ~/.chef/knife.rb, enter your the @chef_server_url@

h2. Credentials

We need to stuff in the AWS credentials (use the "aws console":http://bit.ly/awsconsole to get keys and so forth).

* Create keypairs named 'chef' and 'zaius'. Save them as @~/.hadoop-ec2/keypairs/chef.pem@ and @~/.hadoop-ec2/keypairs/zaius.pem@ respectively, and fix their permissions: @chmod 600 ~/.hadoop-ec2/keypairs/*.pem@.
* In the file @~/.hadoop-ec2/poolparty.yaml@,
** Add your AWS access key and secret access key at the top and again under attributes[:aws]
** Put the domain name of your chef server in the top-level attributes[:chef] section, and again in the pools[:chef][:server][:attributes][:node_name].
** Put the elastic IPs you allocated in the right places (chef server, cluster masters)
** If needed, update the region, availability zone and ec2_url. If you change regions you'll have to change the AMI ids too.

 
h2. Set up your AWS account 

* In your ~/.hadoop-ec2 directory, open the file 'aws_private_setup.sh'. We'll land all the passwords here for starters.

Here's the full bestiary of keys, countersigns, passe-partouts, tokens, shibboleths, watchwords and certificates you'll need. Unless otherwise given, make these changes from within your @~/.hadoop-ec2/@ directory.

* Sign in to the "aws console":http://bit.ly/awsconsole

* Log in to your AWS comsole using your acccount's email and its Amazon password.  You won't use these for anything else in this setup. 
** Consider setting use a "master password" for this Firefox profile. Despite their apparent simplicity, your account email/password are far more important than any of the others: I can use it to invalidate or look up any of the tokens below.
* Near the top of the page, right below "Welcome, Whoever Youare" is your  Account Number (something like "8675-3000-9999").  Record that **without the dashes** in your ~/.hadoop-ec2/aws_private_setup.sh file.
* From the Your Account / Access Keys, find your AWS_ACCESS_KEY_ID (20 characters: something like "12345ABCDE12345ABCDE"). Record it in your ~/.hadoop-ec2/aws_private_setup.sh file.
* From the Your Account / Access Keys, find your AWS_SECRET_ACCESS_KEY (40 characters, something like "gobBBLEDy/goo+kANDlattersandnumbersd00dA"). Record it in your ~/.hadoop-ec2/aws_private_setup.sh file.

* From the Your Account / Security Credentials, in the X.509 Certificates tab, generate a certificate.
** Save your Private Key as @~/.hadoop-ec2/pk-WHATEVER.pem@
** Save your certificate as @~/.hadoop-ec2/cert-WHATEVER.pem@
** make a symlink: @ln -s pk-WHATEVER.pem pk-x509.pem@
** make a symlink: @ln -s cert-WHATEVER.pem cert-x509.pem@
** The long WHATEVER part isn't significant, it's just to identify your certificate against the one in Amazon's possession.
** Note that this is your ONLY CHANCE to save the private key -- Amazon does not keep a copy.

* From the "EC2 Console,":http://bit.ly/awsconsole create a key pair named after your cluster.  Save the private key: @mv ~/Downloads/lemur.pem ~/.hadoop-ec2@

h2. Setting up your credential files

Visit each of the following files and add your credentials:

h3. aws_private_setup.sh

    export AWS_ACCOUNT_ID=123456789012
    export AWS_ACCESS_KEY_ID=12345678901234567890
    export AWS_SECRET_ACCESS_KEY=40UPPERanddowncaseletters+and/andnumbers
    export EC2_PRIVATE_KEY=${HOME}/.hadoop-ec2/pk.pem
    export EC2_CERT=${HOME}/.hadoop-ec2/cert.pem
    export EC2_HOME=/usr/local/share/ec2-api-tools
    
h3. ec2-clusters.cfg

    # 2009-09 Cloudera   Cloudera Hadoop 0.18 32bit Ubuntu AMI: ami-ed59bf84
    # 2009-10 Infochimps Cloudera Hadoop 0.20 32bit Ubuntu AMI: ami-bbd437d2

    [lemur]
    key_name          = lemur
    private_key       = /Users/flip/.hadoop-ec2/lemur.pem
    ami               = ami-ed59bf84
    instance_type     = c1.medium
    availability_zone = us-east-1d
    ssh_options       = -i %(private_key)s -o StrictHostKeyChecking=no

h3. boto

    [Credentials]
    aws_access_key_id     = 12345678901234567890
    aws_secret_access_key = 40UPPERanddowncaseletters+and/andnumbers

h3. s3config.yml (s3sync)

    aws_access_key_id:      12345678901234567890
    aws_secret_access_key:  40UPPERanddowncaseletters+and/andnumbers
    ssl_cert_dir:           /Users/flip/.hadoop-ec2

h2. Setting up Firefox

I use multiple Firefox instances, so that I can run one with just the EC2 plugins and keep my main browser lightweight.  Instructions are here: http://bit.ly/manyfoxen courtesy John Resig. In a nutshell, you're going to make two copies of the program, and choose the profile each time you start.

h3. Install Add-ons

* FoxyProxy: From the *Tolls/Add-ons* menu, search for 'FoxyProxy', install same.  (You want the full version, not the basic).
* S3Fox: From the *Tolls/Add-ons* menu, search for 'S3Fox', install same
* ElasticFox is **not** available through the Add-ons menu.  Instead,
** visit http://bit.ly/elasticfox
** Hit 'Download'
** Select "Allow" from the permissions ribbon at the top of the browser window
** Pick "Install"

Now restart Firefox.

h3. S3Fox

Launch S3Fox from Tools / S3 Organizer.  Hit 'Manage Accounts'. Give some sensible label for the account name, and enter your  AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.

h3. ElasticFox

Launch S3Fox from Tools / ElasticFox.  When prompted, give some sensible label for the account name, and enter your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.

It will take a noticeable amount of time for the plugin to populate.

h3. FoxyProxy

From Tools / Add-ons, select FoxyProxy and hit its Preferences button.
* Set mode to 'Use Proxies based on their pre-defined patterns and priorities"
* Add New Proxy:
** Under Proxy Details, select 'Automatic Proxy configuration URL'
** in the text box that follows, paste in http://github.com/infochimps/hadoop_cluster_chef/raw/master/config/proxy.pac
** Hit test just to check.
** Under 'URL Patterns', whitelist these:
  <notextile>
  *://10.*
  *ec2*.amazonaws.com*
  *ec2.internal*
  *domu*.internal*
  *compute*.internal*
  </notextile>
  
** Under 'URL Patterns', blacklist the URL pattern
  <notextile>
  https://us-*st-1.ec2.amazonaws.com/*
  </notextile>
  
** Under 'General', name the proxy 'Hadoop Cluster Proxy'
** Hit OK to add the proxy. YAY.
* Finally, go to the 'Global Settings' tab and check 'Use SOCKS proxy for DNS lookups'. Let Firefox restart.

(Thanks to this post on  "stylefeeder":http://blog.tech.stylefeeder.com/2009/09/09/foxyproxy-clouder/)

