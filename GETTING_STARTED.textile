h1. Starting From Scratch...

This page walks through how to set up a more or less virgin machine to access the compute cluster infrastructure.

h1. Cluster Chef

This page describes how to use chef to spin up up a compute cluster.

h2. Chef One Time Initial Setup:

We use chef across the board at Infochimps for spinning up machines of different shapes and sizes. This page is not intended to be an introduction to Chef in any way. Rather, using this initial setup guide should be sufficient to spin up and run a //pre-configured// compute cluster only.

1. Set up the agreed upon directory structure.

<pre><code>
mkdir -p sysadmin
mkdir -p backend
</code></pre>

2. Clone the following git repositories that specify cluster configuration, etc.
<pre><code>
cd sysadmin
git clone git@github.com:infochimps/cluster_chef.git
git clone git+ssh://yourcompany.com/gitrepos/ics/sysadmin/infochimps_chef
</code></pre>

3. Make symlinks for chef, poolparty, knife, and broham config files:
<pre><code>
cd ~/.hadoop-ec2 #you should have this already if you followed "Initial Setup"
ln -s examples/poolparty.yaml poolparty.yaml
ln -s examples/knife.rb knife.rb
ln -s poolparty.yaml aws
ln -s ~/.hadoop-ec2 ~/.chef
ln -s ~/.hadoop-ec2 ~/.poolparty
ln -s ~/.hadoop-ec2/poolparty.yaml ~/.configliere/broham.yaml
</code></pre>

4. Install other necessary gems

<pre><code>
sudo gem install configliere broham chef poolparty amazon-ec2 right_aws
</code></pre>

5. Setup Commands

Knife allows you to interact with the chef server via the command line. If it isn't in your path, run:

<pre><code>
sudo ln -s /usr/lib/ruby/gems/1.8/gems/chef-0.9.8/bin/knife ~/bin/knife
</code></pre>
Note that if you're on Mac OS X you'll want to replace {{/usr/lib/ruby/gems}} with {{/Library/Ruby/Gems}}

You'll also want the broham commands in your path

<pre><code>
for cmd in `ls backend/broham/bin | ruby -ne 'puts $_.strip.gsub(/@|\*/,"")'`; do sudo ln -s backend/broham/bin/$cmd $HOME/bin/$cmd; done
</code></pre>

h2. Spin Up the Master Node

I'm going to assume from here on out that you're spinning up the "chimpmark" cluster.

0. Always make sure all relevant git repositories are up to date.

1. Edit {{~/.hadoop-ec2/poolparty.yaml}} to reflect the way you'd like the master node to be spun up. This includes setting the spot price fraction, availability zone, and slave instances among other things. {{~/.hadoop-ec2/notes/pricing.tsv}} has options for instance types.

2. Edit {{clouds/chimpmark_clouds.rb}} to reflect your cluster specifications. Change {{pool 'cluster-name'}} and comment in/out roles for the master node.

3. Edit the role file {{roles/chimpmark_cluster.rb}} and adjust {{:max_map_tasks}} and {{:max_reduce_tasks}} as needed.

4. Update chef with the new role.
<pre><code>
knife role from file roles/chimpmark_cluster.rb
</code></pre>

5. Run the cloud command. This talks directly to AWS and launches the instance. You may have to wait a while if you're using spot pricing. 
<pre><code>
cloud start -n master -c clouds/chimpmark_clouds.rb
</code></pre>

6. Wait for the master to come up completely. This means, ssh into the master node and tail the chef-client logs.

<pre><code>
mj-ssh chimpmark 'ec2-node-whatever'

tail -f -n100 /etc/sv/chef-client/log/main/current
</code></pre>
This should run to completion just fine, ie. runs through all the way to installing and running hadoop, but if you see errors while tailing the logs, restarting chef while on the master node a few times should clear up any issues.
<pre><code>
sudo service chef-client restart
</code></pre>

7. Now that the master node is up and running you should be able to log into the job tracker.

<pre><code>
ssh -i ~/.hadoop-ec2/keypairs/chimpmark.pem -f -N -D 6666 -o StrictHostKeyChecking=no -o "ConnectTimeout=10" -o "ServerAliveInterval=60" -o "ControlPath=none" ubuntu@'ec2-node-whatever'
</code></pre>

8. Format the namenode if the namenode does not come up on the jobtracker.

<pre><code>
for srvice in namenode secondarynamenode jobtracker datanode tasktracker; do sudo service hadoop-0.20-$srvice stop; done
sudo -u hdfs hadoop namenode -format
for srvice in namenode secondarynamenode jobtracker datanode tasktracker; do sudo service hadoop-0.20-$srvice start; done
</code></pre>

9. Change permissions

<pre><code>
hadoop fs -chmod -R g+w /
hadoop fs -chgrp -R hadoop /
for dmn in namenode secondarynamenode jobtracker datanode tasktracker; do sudo service hadoop-0.20-$dmn restart; done
</code></pre>

10. If the namenode ip doesn't show up in the hadoop configuration files
<pre><code>
sudo sed -i -e "s|<value>hdfs://:8020/</value>|<value>hdfs://`hostname -i`:8020</value>|" /etc/hadoop/conf/core-site.xml
sudo sed -i -e "s|<value>:8021/</value>|<value>`hostname -i`:8021</value>|" /etc/hadoop/conf/mapred-site.xml
</code></pre>

h2. Spin up the Slaves

1. Again you'll want to edit {{~/.hadoop-ec2/poolparty.yaml}} to reflect the number of slaves, spot price fraction, etc.

<pre><code>
cloud start -n slave -c clouds/chimpmark_clouds.rb
</code></pre>

2. Once the cluster is up any changes made to {{roles/chimpmark_cluster.rb}} need to be re-roled.

<pre><code>
knife role from file roles/chimpmark_cluster.rb
sudo service chef-client restart
sudo service hadoop-0.20-tasktracker restart
sudo service hadoop-0.20-jobtracker restart
[[/code]]


h1. Other tools


h2. Patched poolparty gem

For the poolparty gem, the infochimps branch has few critical changes (spot pricing and other tweaks). Pull from there

<pre><code>
git clone git@github.com:infochimps/poolparty.git
<pre><code>

Then use rake to create the gem, and then gem to install from the local .gem file just made. These changes aren't vital, so it's OK to skip this step


h2. Cloudera's Hadoop-ec2 Scripts

You'll want these so you can work with Amazon Web Services (AWS) from the command line. Other tools we've built tend to assume you've already got these set up.

1. If it isn't already, make ~/.hadoop-ec2 a clone of the a private repo you won't share with anyone. This will set you up with AWS credentials and various keypairs you'll need:

<pre><code>
mkdir -p ~/.hadoop-ec2
cd ~/.hadoop-ec2
git init
git remote add origin git+ssh://yourcompany.com/gitrepos/hadoop-ec2.git
git pull
</code></pre>

4. Hadoop-ec2 python dependencies

the {{hadoop-ec2}} command has a couple easy to overlook dependencies.

<pre><code>
sudo apt-get install python-setuptools
sudo easy_install boto
sudo easy_install simplejson
</code></pre>

h2. Ec2 ami/api tools

<pre><code>
sudo apt-get install ec2-ami-tools
sudo apt-get install ec2-api-tools
</code></pre>

**These try to install ruby 1.8 on the latest ubuntu--BAD if you want to use 1.9**

If you're using Mac OS X you'll have to replace {{sudo apt-get install python-setuptools}} with {{sudo fink install setuptools-py26}}. See [http://www.finkproject.org/download/index.php?phpLang=en Fink].

h2. Setting up Firefox

Now that you've got the hadoop-ec2 scripts up and running you need to setup Firefox. This will allow you to view the Hadoop jobtracker and namenode pages as well as other pages that require a proxy (eg. Azkaban and Bigsheets).

1. Install the FoxyProxy add-on. Go to //Tools// >> //Add-ons// and search for FoxyProxy. Click to install FoxyProxy standard. This will require a restart.

2. Setup FoxyProxy. Go to //Tools// >> //FoxyProxy Standard// >> //Options//
i.   Set it to 'Use Proxies based on their pre-defined patterns and priorities'
ii.  Create a new proxy, called 'EC2 Socks Proxy' or something
iii. Automatic proxy configuration URL: http://github.com/infochimps/cluster_chef/raw/master/config/proxy.pac
iv. Under 'General', check yes for 'Perform remote DNS lookups on host'
v.  Add the following URL patterns as 'whitelist' using 'Wildcards' (not regular expression):
           *.compute.internal*
           *ec2.internal*
           *domu*.internal*
           *ec2*.amazonaws.com*
           *://10.*
      And this one as blacklist:
          * https://us-*st-1.ec2.amazonaws.com/*

h2. Public Keys

Your public key will need to be on the nfs in order for you to log into the cluster.

<pre><code>
ssh-keygen
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys2
</code></pre>

Copy {{authorized_keys2}} in whatever way is easiest to ~/.ssh on the nfs.

+ RUBYGEMS WARNING

You probably already have (good for you if you haven't) fucked up your rubygems. **DO NOT USE apt-get!**

<pre><code>
# THIS IS BAD, installs oldy goldy rubygems in /var/lib/ blah blah
sudo apt-get install ruby gems
</code></pre>

If you accidentally were lazy and installed rubygems via apt-get, all is not lost. Here is the procedure to fix it. It may take a few minutes:

<pre><code>
# Remove fucked up gems dir
gem list > gems.txt
sudo apt-get remove rubygems
sudo rm -r /var/lib/gems

# Install rubygems the right way
cd ~
wget http://rubyforge.org/frs/download.php/70696/rubygems-1.3.7.tgz
tar xzvf rubygems-1.3.7.tgz
cd rubygems-1.3.7
sudo ruby setup.rb

# Reinstall your gems
for foo in `cat gems.txt | ruby -ne 'puts $_.strip.split.first'`; do sudo gem install --no-ri --no-rdoc $foo; done
</code></pre>
